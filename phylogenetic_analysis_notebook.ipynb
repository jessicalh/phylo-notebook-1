{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Phylogenetic Analysis Pipeline\n",
    "## Efficient homolog search, alignment, and tree construction using PDB structures\n",
    "\n",
    "### Features:\n",
    "- GPU-accelerated MMseqs2 homolog search\n",
    "- FAMSA2 ultra-fast alignment\n",
    "- IQ-TREE phylogenetic inference\n",
    "- Comprehensive logging and error handling\n",
    "- Google Drive integration for results\n",
    "- Interactive visualizations and CSV outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 1: Complete Installation Suite with Logging\n",
    "All installations happen here with explicit error catching and verbose logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE INSTALLATION CELL WITH LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize installation log\n",
    "install_log = []\n",
    "install_start = datetime.now()\n",
    "\n",
    "def log_install(package, status, message=\"\", error=None):\n",
    "    \"\"\"Log installation attempts with timestamp\"\"\"\n",
    "    entry = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'package': package,\n",
    "        'status': status,\n",
    "        'message': message,\n",
    "        'error': str(error) if error else None\n",
    "    }\n",
    "    install_log.append(entry)\n",
    "    \n",
    "    # Print colored output\n",
    "    if status == 'success':\n",
    "        print(f\"‚úÖ {package}: {message}\")\n",
    "    elif status == 'failed':\n",
    "        print(f\"‚ùå {package}: {message}\")\n",
    "        if error:\n",
    "            print(f\"   Error details: {error}\")\n",
    "    else:\n",
    "        print(f\"üîÑ {package}: {message}\")\n",
    "    return entry\n",
    "\n",
    "def safe_install(package_name, pip_package=None, test_import=None):\n",
    "    \"\"\"Safely install a package with error handling\"\"\"\n",
    "    pip_package = pip_package or package_name\n",
    "    test_import = test_import or package_name.replace('-', '_')\n",
    "    \n",
    "    try:\n",
    "        log_install(package_name, 'installing', f'Starting installation of {pip_package}')\n",
    "        \n",
    "        # Run pip install\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, '-m', 'pip', 'install', pip_package, '--upgrade'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"pip install failed: {result.stderr}\")\n",
    "        \n",
    "        # Test import\n",
    "        exec(f\"import {test_import}\")\n",
    "        \n",
    "        # Get version if possible\n",
    "        try:\n",
    "            version = subprocess.run(\n",
    "                [sys.executable, '-c', f\"import {test_import}; print({test_import}.__version__)\"],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            ).stdout.strip()\n",
    "            log_install(package_name, 'success', f'Installed version {version}')\n",
    "        except:\n",
    "            log_install(package_name, 'success', 'Installed (version unknown)')\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        log_install(package_name, 'failed', 'Installation timeout', 'Timeout after 300 seconds')\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        log_install(package_name, 'failed', 'Installation failed', e)\n",
    "        return False\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING COMPREHENSIVE INSTALLATION PROCESS\")\n",
    "print(f\"Start time: {install_start}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# CORE SCIENTIFIC PACKAGES\n",
    "# ============================================================================\n",
    "print(\"\\nüì¶ Installing Core Scientific Packages...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "core_packages = [\n",
    "    ('numpy', 'numpy', 'numpy'),\n",
    "    ('pandas', 'pandas', 'pandas'),\n",
    "    ('matplotlib', 'matplotlib', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn', 'seaborn'),\n",
    "    ('scipy', 'scipy', 'scipy'),\n",
    "]\n",
    "\n",
    "for package, pip_name, import_name in core_packages:\n",
    "    safe_install(package, pip_name, import_name)\n",
    "\n",
    "# ============================================================================\n",
    "# BIOINFORMATICS PACKAGES\n",
    "# ============================================================================\n",
    "print(\"\\nüß¨ Installing Bioinformatics Packages...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "bio_packages = [\n",
    "    ('biopython', 'biopython', 'Bio'),\n",
    "    ('pyfamsa', 'pyfamsa', 'pyfamsa'),\n",
    "    ('dendropy', 'dendropy', 'dendropy'),\n",
    "    ('ete3', 'ete3', 'ete3'),\n",
    "]\n",
    "\n",
    "for package, pip_name, import_name in bio_packages:\n",
    "    safe_install(package, pip_name, import_name)\n",
    "\n",
    "# ============================================================================\n",
    "# PHYLOGENETIC ANALYSIS PACKAGES\n",
    "# ============================================================================\n",
    "print(\"\\nüå≥ Installing Phylogenetic Analysis Packages...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Special handling for piqtree (IQ-TREE Python wrapper)\n",
    "try:\n",
    "    log_install('piqtree', 'installing', 'Attempting to install IQ-TREE Python wrapper')\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', 'piqtree'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=300\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        log_install('piqtree', 'success', 'IQ-TREE Python wrapper installed')\n",
    "    else:\n",
    "        # Fallback: install IQ-TREE binary directly\n",
    "        log_install('piqtree', 'warning', 'piqtree not available, will use IQ-TREE binary')\n",
    "except Exception as e:\n",
    "    log_install('piqtree', 'warning', f'Will use IQ-TREE binary instead: {e}')\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION AND UI PACKAGES\n",
    "# ============================================================================\n",
    "print(\"\\nüé® Installing Visualization and UI Packages...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "viz_packages = [\n",
    "    ('plotly', 'plotly', 'plotly'),\n",
    "    ('ipywidgets', 'ipywidgets', 'ipywidgets'),\n",
    "    ('tqdm', 'tqdm', 'tqdm'),\n",
    "]\n",
    "\n",
    "for package, pip_name, import_name in viz_packages:\n",
    "    safe_install(package, pip_name, import_name)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY PACKAGES\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Installing Utility Packages...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "utility_packages = [\n",
    "    ('requests', 'requests', 'requests'),\n",
    "    ('wget', 'wget', 'wget'),\n",
    "    ('gdown', 'gdown', 'gdown'),\n",
    "]\n",
    "\n",
    "for package, pip_name, import_name in utility_packages:\n",
    "    safe_install(package, pip_name, import_name)\n",
    "\n",
    "# ============================================================================\n",
    "# BINARY TOOLS INSTALLATION\n",
    "# ============================================================================\n",
    "print(\"\\n‚öôÔ∏è Installing Binary Tools...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Install MMseqs2\n",
    "try:\n",
    "    log_install('MMseqs2', 'installing', 'Downloading MMseqs2 binary')\n",
    "    \n",
    "    # Create tools directory\n",
    "    os.makedirs('/content/tools', exist_ok=True)\n",
    "    \n",
    "    # Download MMseqs2\n",
    "    mmseqs_url = 'https://github.com/soedinglab/MMseqs2/releases/latest/download/mmseqs-linux-avx2.tar.gz'\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        f'cd /content/tools && wget -q {mmseqs_url} && tar xzf mmseqs-linux-avx2.tar.gz',\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        # Test MMseqs2\n",
    "        test = subprocess.run(\n",
    "            '/content/tools/mmseqs/bin/mmseqs version',\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        version = test.stdout.strip() if test.returncode == 0 else 'unknown'\n",
    "        log_install('MMseqs2', 'success', f'Binary installed, version: {version}')\n",
    "    else:\n",
    "        raise Exception(f\"Download failed: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log_install('MMseqs2', 'failed', 'Could not install MMseqs2 binary', e)\n",
    "\n",
    "# Install IQ-TREE\n",
    "try:\n",
    "    log_install('IQ-TREE', 'installing', 'Downloading IQ-TREE binary')\n",
    "    \n",
    "    iqtree_url = 'https://github.com/iqtree/iqtree2/releases/download/v2.3.6/iqtree-2.3.6-Linux-intel.tar.gz'\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        f'cd /content/tools && wget -q {iqtree_url} && tar xzf iqtree-2.3.6-Linux-intel.tar.gz',\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        # Create symlink for easier access\n",
    "        subprocess.run('ln -sf /content/tools/iqtree-2.3.6-Linux-intel/bin/iqtree2 /content/tools/iqtree2', shell=True)\n",
    "        \n",
    "        # Test IQ-TREE\n",
    "        test = subprocess.run(\n",
    "            '/content/tools/iqtree2 --version',\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        version = test.stdout.split('\\n')[0] if test.returncode == 0 else 'unknown'\n",
    "        log_install('IQ-TREE', 'success', f'Binary installed: {version}')\n",
    "    else:\n",
    "        raise Exception(f\"Download failed: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log_install('IQ-TREE', 'failed', 'Could not install IQ-TREE binary', e)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY GPU AVAILABILITY\n",
    "# ============================================================================\n",
    "print(\"\\nüéÆ Checking GPU Availability...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "try:\n",
    "    gpu_check = subprocess.run(\n",
    "        'nvidia-smi --query-gpu=name,memory.total --format=csv,noheader',\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if gpu_check.returncode == 0:\n",
    "        gpu_info = gpu_check.stdout.strip()\n",
    "        log_install('GPU', 'success', f'GPU detected: {gpu_info}')\n",
    "    else:\n",
    "        log_install('GPU', 'warning', 'No GPU detected, will use CPU')\n",
    "except Exception as e:\n",
    "    log_install('GPU', 'warning', f'Could not check GPU: {e}')\n",
    "\n",
    "# ============================================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# ============================================================================\n",
    "print(\"\\n‚òÅÔ∏è Mounting Google Drive...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create log directory\n",
    "    log_dir = '/content/drive/MyDrive/phylo_logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Save installation log\n",
    "    log_file = f'{log_dir}/installation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(install_log, f, indent=2)\n",
    "    \n",
    "    log_install('Google Drive', 'success', f'Mounted and log saved to {log_file}')\n",
    "    \n",
    "except Exception as e:\n",
    "    log_install('Google Drive', 'failed', 'Could not mount Google Drive', e)\n",
    "    print(\"‚ö†Ô∏è Continuing without Drive access, results will be saved locally\")\n",
    "\n",
    "# ============================================================================\n",
    "# INSTALLATION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INSTALLATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count successes and failures\n",
    "success_count = sum(1 for entry in install_log if entry['status'] == 'success')\n",
    "failed_count = sum(1 for entry in install_log if entry['status'] == 'failed')\n",
    "warning_count = sum(1 for entry in install_log if entry['status'] == 'warning')\n",
    "\n",
    "print(f\"‚úÖ Successful installations: {success_count}\")\n",
    "print(f\"‚ö†Ô∏è Warnings: {warning_count}\")\n",
    "print(f\"‚ùå Failed installations: {failed_count}\")\n",
    "print(f\"‚è±Ô∏è Total time: {datetime.now() - install_start}\")\n",
    "\n",
    "if failed_count > 0:\n",
    "    print(\"\\n‚ùå Failed packages:\")\n",
    "    for entry in install_log:\n",
    "        if entry['status'] == 'failed':\n",
    "            print(f\"  - {entry['package']}: {entry['message']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Installation phase complete. Proceed to imports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 2: Import All Libraries with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE IMPORTS WITH VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import_log = []\n",
    "import_start = datetime.now()\n",
    "\n",
    "def safe_import(module_name, import_as=None, from_module=None):\n",
    "    \"\"\"Safely import a module with error handling\"\"\"\n",
    "    try:\n",
    "        if from_module:\n",
    "            exec(f\"from {from_module} import {module_name}\")\n",
    "            globals()[module_name] = eval(module_name)\n",
    "        elif import_as:\n",
    "            exec(f\"import {module_name} as {import_as}\")\n",
    "            globals()[import_as] = eval(import_as)\n",
    "        else:\n",
    "            exec(f\"import {module_name}\")\n",
    "            globals()[module_name] = eval(module_name)\n",
    "        \n",
    "        # Get version if available\n",
    "        version_str = \"unknown\"\n",
    "        try:\n",
    "            if import_as:\n",
    "                version_str = eval(f\"{import_as}.__version__\")\n",
    "            else:\n",
    "                version_str = eval(f\"{module_name}.__version__\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        import_log.append({'module': module_name, 'status': 'success', 'version': version_str})\n",
    "        print(f\"‚úÖ Imported {module_name} (version: {version_str})\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        import_log.append({'module': module_name, 'status': 'failed', 'error': str(e)})\n",
    "        print(f\"‚ùå Failed to import {module_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"IMPORTING ALL REQUIRED LIBRARIES\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STANDARD LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "print(\"üìö Standard Library Imports...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "safe_import('os')\n",
    "safe_import('sys')\n",
    "safe_import('json')\n",
    "safe_import('logging')\n",
    "safe_import('subprocess')\n",
    "safe_import('shutil')\n",
    "safe_import('tempfile')\n",
    "safe_import('warnings')\n",
    "safe_import('traceback')\n",
    "safe_import('datetime', from_module='datetime')\n",
    "safe_import('Path', from_module='pathlib')\n",
    "safe_import('defaultdict', from_module='collections')\n",
    "safe_import('Counter', from_module='collections')\n",
    "\n",
    "# ============================================================================\n",
    "# SCIENTIFIC COMPUTING IMPORTS\n",
    "# ============================================================================\n",
    "print(\"\\nüî¨ Scientific Computing Libraries...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "safe_import('numpy', import_as='np')\n",
    "safe_import('pandas', import_as='pd')\n",
    "safe_import('scipy')\n",
    "safe_import('stats', from_module='scipy')\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION IMPORTS\n",
    "# ============================================================================\n",
    "print(\"\\nüìä Visualization Libraries...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "safe_import('matplotlib.pyplot', import_as='plt')\n",
    "safe_import('seaborn', import_as='sns')\n",
    "safe_import('plotly.graph_objects', import_as='go')\n",
    "safe_import('plotly.express', import_as='px')\n",
    "\n",
    "# Set visualization defaults\n",
    "try:\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    warnings.filterwarnings('ignore')\n",
    "    print(\"‚úÖ Visualization defaults configured\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not set visualization defaults: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BIOINFORMATICS IMPORTS\n",
    "# ============================================================================\n",
    "print(\"\\nüß¨ Bioinformatics Libraries...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Biopython imports\n",
    "safe_import('Bio')\n",
    "safe_import('SeqIO', from_module='Bio')\n",
    "safe_import('Seq', from_module='Bio.Seq')\n",
    "safe_import('SeqRecord', from_module='Bio.SeqRecord')\n",
    "safe_import('PDBList', from_module='Bio.PDB')\n",
    "safe_import('PDBParser', from_module='Bio.PDB')\n",
    "safe_import('AlignIO', from_module='Bio')\n",
    "safe_import('Phylo', from_module='Bio')\n",
    "safe_import('DistanceCalculator', from_module='Bio.Phylo.TreeConstruction')\n",
    "safe_import('DistanceTreeConstructor', from_module='Bio.Phylo.TreeConstruction')\n",
    "\n",
    "# FAMSA alignment\n",
    "try:\n",
    "    safe_import('pyfamsa')\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è pyfamsa not available, will use fallback alignment method\")\n",
    "\n",
    "# Phylogenetic analysis\n",
    "safe_import('dendropy')\n",
    "try:\n",
    "    safe_import('ete3')\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è ete3 not available, will use alternative tree visualization\")\n",
    "\n",
    "# ============================================================================\n",
    "# UI AND UTILITY IMPORTS\n",
    "# ============================================================================\n",
    "print(\"\\nüõ†Ô∏è UI and Utility Libraries...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "safe_import('widgets', from_module='ipywidgets')\n",
    "safe_import('display', from_module='IPython.display')\n",
    "safe_import('HTML', from_module='IPython.display')\n",
    "safe_import('tqdm.notebook', import_as='tqdm_notebook')\n",
    "safe_import('requests')\n",
    "safe_import('wget')\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE COLAB SPECIFIC IMPORTS\n",
    "# ============================================================================\n",
    "print(\"\\n‚òÅÔ∏è Google Colab Utilities...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "try:\n",
    "    safe_import('drive', from_module='google.colab')\n",
    "    safe_import('files', from_module='google.colab')\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab environment\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è Not in Colab environment, some features may be limited\")\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP LOGGING SYSTEM\n",
    "# ============================================================================\n",
    "print(\"\\nüìù Setting up Logging System...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s | %(levelname)s | %(funcName)s | Line:%(lineno)d | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create main logger\n",
    "logger = logging.getLogger('phylo_pipeline')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Remove default handler to avoid duplication\n",
    "logger.handlers = []\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_formatter = logging.Formatter(\n",
    "    '%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "console_handler.setFormatter(console_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File handler (if Drive is mounted)\n",
    "if IN_COLAB and os.path.exists('/content/drive/MyDrive'):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = f'/content/drive/MyDrive/phylo_logs/run_{timestamp}.log'\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s | %(levelname)s | %(funcName)s | Line:%(lineno)d | %(message)s'\n",
    "    )\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    print(f\"‚úÖ Logging to file: {log_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è File logging disabled (Drive not available)\")\n",
    "\n",
    "logger.info(\"Logging system initialized\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPORT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "success_imports = sum(1 for entry in import_log if entry['status'] == 'success')\n",
    "failed_imports = sum(1 for entry in import_log if entry['status'] == 'failed')\n",
    "\n",
    "print(f\"‚úÖ Successful imports: {success_imports}\")\n",
    "print(f\"‚ùå Failed imports: {failed_imports}\")\n",
    "print(f\"‚è±Ô∏è Import time: {datetime.now() - import_start}\")\n",
    "\n",
    "if failed_imports > 0:\n",
    "    print(\"\\n‚ùå Failed imports (may affect functionality):\")\n",
    "    for entry in import_log:\n",
    "        if entry['status'] == 'failed':\n",
    "            print(f\"  - {entry['module']}: {entry.get('error', 'Unknown error')}\")\n",
    "    print(\"\\n‚ö†Ô∏è Some features may be limited due to import failures\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All imports successful! Ready to proceed.\")\n",
    "\n",
    "# Save import log if Drive is available\n",
    "if IN_COLAB and os.path.exists('/content/drive/MyDrive'):\n",
    "    import_log_file = f'/content/drive/MyDrive/phylo_logs/imports_{timestamp}.json'\n",
    "    with open(import_log_file, 'w') as f:\n",
    "        json.dump(import_log, f, indent=2)\n",
    "    print(f\"\\nüíæ Import log saved to: {import_log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 3: Helper Functions and Exception Handling Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS AND EXCEPTION HANDLING FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"Setting up helper functions and exception handling framework\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXCEPTION HANDLING DECORATORS\n",
    "# ============================================================================\n",
    "\n",
    "def safe_cell_execution(func_name):\n",
    "    \"\"\"Decorator for cell-level exception handling\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                logger.info(f\"Starting cell operation: {func_name}\")\n",
    "                result = func(*args, **kwargs)\n",
    "                logger.info(f\"Successfully completed: {func_name}\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in {func_name}: {str(e)}\", exc_info=True)\n",
    "                print(f\"\\n‚ùå Error in {func_name}: {str(e)}\")\n",
    "                print(\"Check log file for full traceback\")\n",
    "                \n",
    "                # Save error details to Drive if available\n",
    "                if IN_COLAB and os.path.exists('/content/drive/MyDrive'):\n",
    "                    error_file = f'/content/drive/MyDrive/phylo_logs/error_{timestamp}_{func_name}.txt'\n",
    "                    with open(error_file, 'w') as f:\n",
    "                        f.write(f\"Error in {func_name}\\n\")\n",
    "                        f.write(f\"Time: {datetime.now()}\\n\")\n",
    "                        f.write(f\"Error: {str(e)}\\n\\n\")\n",
    "                        f.write(\"Full traceback:\\n\")\n",
    "                        f.write(traceback.format_exc())\n",
    "                    print(f\"üìù Error details saved to: {error_file}\")\n",
    "                \n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def safe_operation(op_name):\n",
    "    \"\"\"Decorator for operation-level exception handling\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                logger.debug(f\"Executing operation: {op_name}\")\n",
    "                result = func(*args, **kwargs)\n",
    "                logger.debug(f\"Operation successful: {op_name}\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Operation failed {op_name}: {e}\")\n",
    "                return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# ============================================================================\n",
    "# FILE MANAGEMENT UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class FileManager:\n",
    "    \"\"\"Centralized file management with logging\"\"\"\n",
    "    \n",
    "    def __init__(self, pdb_id, timestamp):\n",
    "        self.pdb_id = pdb_id\n",
    "        self.timestamp = timestamp\n",
    "        self.setup_directories()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create directory structure\"\"\"\n",
    "        if IN_COLAB and os.path.exists('/content/drive/MyDrive'):\n",
    "            self.base_dir = f'/content/drive/MyDrive/phylo_results/{self.timestamp}_{self.pdb_id}'\n",
    "        else:\n",
    "            self.base_dir = f'/content/phylo_results/{self.timestamp}_{self.pdb_id}'\n",
    "        \n",
    "        self.dirs = {\n",
    "            'logs': f'{self.base_dir}/logs',\n",
    "            'sequences': f'{self.base_dir}/sequences',\n",
    "            'alignments': f'{self.base_dir}/alignments',\n",
    "            'trees': f'{self.base_dir}/trees',\n",
    "            'visualizations': f'{self.base_dir}/visualizations',\n",
    "            'csv_outputs': f'{self.base_dir}/csv_outputs',\n",
    "            'checkpoints': f'{self.base_dir}/checkpoints'\n",
    "        }\n",
    "        \n",
    "        for dir_name, dir_path in self.dirs.items():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            logger.debug(f\"Created directory: {dir_path}\")\n",
    "    \n",
    "    def save_file(self, content, filename, subdir=''):\n",
    "        \"\"\"Save file with logging\"\"\"\n",
    "        if subdir and subdir in self.dirs:\n",
    "            filepath = f\"{self.dirs[subdir]}/{filename}\"\n",
    "        else:\n",
    "            filepath = f\"{self.base_dir}/{filename}\"\n",
    "        \n",
    "        try:\n",
    "            if isinstance(content, str):\n",
    "                with open(filepath, 'w') as f:\n",
    "                    f.write(content)\n",
    "            elif isinstance(content, bytes):\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(content)\n",
    "            elif isinstance(content, pd.DataFrame):\n",
    "                content.to_csv(filepath, index=False)\n",
    "            else:\n",
    "                with open(filepath, 'w') as f:\n",
    "                    json.dump(content, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"Saved file: {filepath}\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save {filename}: {e}\")\n",
    "            return None\n",
    "\n",
    "# ============================================================================\n",
    "# PROGRESS TRACKING\n",
    "# ============================================================================\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Track and display pipeline progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps = [\n",
    "            'PDB Download',\n",
    "            'Sequence Extraction',\n",
    "            'Homolog Search',\n",
    "            'Alignment',\n",
    "            'Tree Construction',\n",
    "            'Visualization',\n",
    "            'Report Generation'\n",
    "        ]\n",
    "        self.current_step = 0\n",
    "        self.step_times = {}\n",
    "    \n",
    "    def start_step(self, step_name):\n",
    "        \"\"\"Mark step as started\"\"\"\n",
    "        self.step_times[step_name] = {'start': datetime.now()}\n",
    "        logger.info(f\"Starting step: {step_name}\")\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîÑ STEP {self.current_step + 1}/{len(self.steps)}: {step_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    def complete_step(self, step_name):\n",
    "        \"\"\"Mark step as completed\"\"\"\n",
    "        if step_name in self.step_times:\n",
    "            self.step_times[step_name]['end'] = datetime.now()\n",
    "            duration = self.step_times[step_name]['end'] - self.step_times[step_name]['start']\n",
    "            logger.info(f\"Completed step: {step_name} (Duration: {duration})\")\n",
    "            print(f\"‚úÖ Completed: {step_name} (Time: {duration})\")\n",
    "            self.current_step += 1\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get execution summary\"\"\"\n",
    "        summary = []\n",
    "        for step_name, times in self.step_times.items():\n",
    "            if 'end' in times:\n",
    "                duration = times['end'] - times['start']\n",
    "                summary.append({\n",
    "                    'Step': step_name,\n",
    "                    'Duration': str(duration),\n",
    "                    'Status': 'Completed'\n",
    "                })\n",
    "            else:\n",
    "                summary.append({\n",
    "                    'Step': step_name,\n",
    "                    'Duration': 'N/A',\n",
    "                    'Status': 'In Progress'\n",
    "                })\n",
    "        return pd.DataFrame(summary)\n",
    "\n",
    "# ============================================================================\n",
    "# CHECKPOINT SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Save and restore pipeline state\"\"\"\n",
    "    \n",
    "    def __init__(self, file_manager):\n",
    "        self.file_manager = file_manager\n",
    "        self.checkpoints = {}\n",
    "    \n",
    "    def save_checkpoint(self, name, data):\n",
    "        \"\"\"Save checkpoint data\"\"\"\n",
    "        try:\n",
    "            checkpoint_file = f\"checkpoint_{name}_{datetime.now().strftime('%H%M%S')}.json\"\n",
    "            filepath = self.file_manager.save_file(data, checkpoint_file, 'checkpoints')\n",
    "            self.checkpoints[name] = filepath\n",
    "            logger.info(f\"Saved checkpoint: {name}\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save checkpoint {name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_checkpoint(self, name):\n",
    "        \"\"\"Load checkpoint data\"\"\"\n",
    "        try:\n",
    "            if name in self.checkpoints:\n",
    "                with open(self.checkpoints[name], 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                logger.info(f\"Loaded checkpoint: {name}\")\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load checkpoint {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize global objects\n",
    "# ============================================================================\n",
    "\n",
    "# These will be initialized when the analysis starts\n",
    "file_manager = None\n",
    "progress_tracker = None\n",
    "checkpoint_manager = None\n",
    "\n",
    "print(\"‚úÖ Helper functions and exception handling framework initialized\")\n",
    "logger.info(\"Framework initialization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 4: User Input Form and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER INPUT FORM AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@safe_cell_execution(\"User Input Form\")\n",
    "def create_input_form():\n",
    "    \"\"\"Create and display interactive input form\"\"\"\n",
    "    \n",
    "    logger.info(\"Creating user input form\")\n",
    "    \n",
    "    # Create form widgets\n",
    "    style = {'description_width': '150px'}\n",
    "    layout = widgets.Layout(width='500px')\n",
    "    \n",
    "    # PDB ID input\n",
    "    pdb_input = widgets.Text(\n",
    "        value='1UBQ',\n",
    "        placeholder='Enter PDB ID (e.g., 1UBQ)',\n",
    "        description='PDB ID:',\n",
    "        style=style,\n",
    "        layout=layout\n",
    "    )\n",
    "    \n",
    "    # E-value threshold\n",
    "    evalue_input = widgets.FloatLogSlider(\n",
    "        value=1e-5,\n",
    "        min=-10,\n",
    "        max=0,\n",
    "        step=0.5,\n",
    "        description='E-value threshold:',\n",
    "        style=style,\n",
    "        layout=layout\n",
    "    )\n",
    "    \n",
    "    # Maximum sequences\n",
    "    max_seqs_input = widgets.IntSlider(\n",
    "        value=5000,\n",
    "        min=100,\n",
    "        max=10000,\n",
    "        step=100,\n",
    "        description='Max sequences:',\n",
    "        style=style,\n",
    "        layout=layout\n",
    "    )\n",
    "    \n",
    "    # Coverage threshold\n",
    "    coverage_input = widgets.IntSlider(\n",
    "        value=50,\n",
    "        min=10,\n",
    "        max=100,\n",
    "        step=5,\n",
    "        description='Min coverage (%):',\n",
    "        style=style,\n",
    "        layout=layout\n",
    "    )\n",
    "    \n",
    "    # Database selection\n",
    "    database_input = widgets.Dropdown(\n",
    "        options=['UniRef50 (Fast)', 'UniRef90 (Comprehensive)', 'UniRef100 (Complete)'],\n",
    "        value='UniRef50 (Fast)',\n",
    "        description='Database:',\n",
    "        style=style,\n",
    "        layout=layout\n",
    "    )\n",
    "    \n",
    "    # Verbose mode\n",
    "    verbose_input = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Verbose logging',\n",
    "        style=style\n",
    "    )\n",
    "    \n",
    "    # Test mode\n",
    "    test_mode_input = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Test mode (small dataset)',\n",
    "        style=style\n",
    "    )\n",
    "    \n",
    "    # Output display\n",
    "    output_display = widgets.Output()\n",
    "    \n",
    "    # Run button\n",
    "    run_button = widgets.Button(\n",
    "        description='Run Analysis',\n",
    "        button_style='success',\n",
    "        icon='play',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    # Validation function\n",
    "    def validate_and_run(b):\n",
    "        with output_display:\n",
    "            output_display.clear_output()\n",
    "            \n",
    "            # Validate PDB ID\n",
    "            pdb_id = pdb_input.value.strip().upper()\n",
    "            if len(pdb_id) != 4:\n",
    "                print(\"‚ùå Invalid PDB ID. Must be 4 characters (e.g., 1UBQ)\")\n",
    "                return\n",
    "            \n",
    "            # Store configuration\n",
    "            global config\n",
    "            config = {\n",
    "                'pdb_id': pdb_id,\n",
    "                'evalue': evalue_input.value,\n",
    "                'max_sequences': max_seqs_input.value,\n",
    "                'min_coverage': coverage_input.value,\n",
    "                'database': database_input.value.split(' ')[0],\n",
    "                'verbose': verbose_input.value,\n",
    "                'test_mode': test_mode_input.value,\n",
    "                'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            }\n",
    "            \n",
    "            # Initialize managers\n",
    "            global file_manager, progress_tracker, checkpoint_manager\n",
    "            file_manager = FileManager(config['pdb_id'], config['timestamp'])\n",
    "            progress_tracker = ProgressTracker()\n",
    "            checkpoint_manager = CheckpointManager(file_manager)\n",
    "            \n",
    "            # Save configuration\n",
    "            config_file = file_manager.save_file(config, 'config.json')\n",
    "            \n",
    "            # Display configuration\n",
    "            print(\"‚úÖ Configuration validated and saved!\")\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ANALYSIS CONFIGURATION\")\n",
    "            print(\"=\"*50)\n",
    "            for key, value in config.items():\n",
    "                print(f\"{key:15}: {value}\")\n",
    "            print(\"\\nOutput directory: \", file_manager.base_dir)\n",
    "            print(\"\\n‚úÖ Ready to proceed with analysis!\")\n",
    "            print(\"Run the next cells to start the pipeline.\")\n",
    "            \n",
    "            logger.info(f\"Configuration set: {config}\")\n",
    "    \n",
    "    run_button.on_click(validate_and_run)\n",
    "    \n",
    "    # Create form layout\n",
    "    form = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üß¨ Phylogenetic Analysis Configuration</h3>\"),\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        pdb_input,\n",
    "        evalue_input,\n",
    "        max_seqs_input,\n",
    "        coverage_input,\n",
    "        database_input,\n",
    "        widgets.HBox([verbose_input, test_mode_input]),\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        run_button,\n",
    "        output_display\n",
    "    ])\n",
    "    \n",
    "    return form\n",
    "\n",
    "# Display the form\n",
    "form = create_input_form()\n",
    "display(form)\n",
    "\n",
    "# Initialize config variable\n",
    "config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 5: PDB Download and Sequence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PDB DOWNLOAD AND SEQUENCE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "@safe_cell_execution(\"PDB Download and Sequence Extraction\")\n",
    "def download_and_extract_sequences():\n",
    "    \"\"\"Download PDB file and extract sequences\"\"\"\n",
    "    \n",
    "    if not config:\n",
    "        raise ValueError(\"Please run the configuration cell first!\")\n",
    "    \n",
    "    progress_tracker.start_step('PDB Download')\n",
    "    \n",
    "    pdb_id = config['pdb_id']\n",
    "    logger.info(f\"Downloading PDB {pdb_id}\")\n",
    "    \n",
    "    sequences = {}\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Use Biopython PDBList\n",
    "        pdbl = PDBList(verbose=False)\n",
    "        pdb_file = pdbl.retrieve_pdb_file(\n",
    "            pdb_id, \n",
    "            pdir='/content/temp',\n",
    "            file_format='pdb'\n",
    "        )\n",
    "        logger.info(f\"Downloaded PDB file: {pdb_file}\")\n",
    "        \n",
    "        # Parse PDB and extract sequences\n",
    "        with open(pdb_file, 'r') as f:\n",
    "            for record in SeqIO.parse(f, 'pdb-atom'):\n",
    "                chain_id = record.id.split(':')[1] if ':' in record.id else record.id\n",
    "                sequences[f\"{pdb_id}_{chain_id}\"] = str(record.seq)\n",
    "                logger.debug(f\"Extracted chain {chain_id}: {len(record.seq)} residues\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Method 1 failed: {e}, trying alternative method\")\n",
    "        \n",
    "        # Method 2: Use REST API\n",
    "        try:\n",
    "            # Get structure info\n",
    "            api_url = f\"https://data.rcsb.org/rest/v1/core/entry/{pdb_id}\"\n",
    "            response = requests.get(api_url)\n",
    "            response.raise_for_status()\n",
    "            structure_info = response.json()\n",
    "            \n",
    "            # Get polymer sequences\n",
    "            polymer_url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/1\"\n",
    "            response = requests.get(polymer_url)\n",
    "            response.raise_for_status()\n",
    "            polymer_data = response.json()\n",
    "            \n",
    "            # Extract sequence\n",
    "            if 'entity_poly' in polymer_data:\n",
    "                sequence = polymer_data['entity_poly']['pdbx_seq_one_letter_code_can']\n",
    "                sequences[f\"{pdb_id}_A\"] = sequence.replace('\\n', '')\n",
    "                logger.info(f\"Retrieved sequence via API: {len(sequence)} residues\")\n",
    "        \n",
    "        except Exception as e2:\n",
    "            logger.error(f\"Both methods failed: {e2}\")\n",
    "            raise Exception(f\"Could not retrieve sequences for PDB {pdb_id}\")\n",
    "    \n",
    "    progress_tracker.complete_step('PDB Download')\n",
    "    progress_tracker.start_step('Sequence Extraction')\n",
    "    \n",
    "    # Save sequences\n",
    "    if sequences:\n",
    "        # Create FASTA file\n",
    "        fasta_content = \"\"\n",
    "        for seq_id, seq in sequences.items():\n",
    "            fasta_content += f\">{seq_id}\\n{seq}\\n\"\n",
    "        \n",
    "        query_file = file_manager.save_file(fasta_content, 'query_sequences.fasta', 'sequences')\n",
    "        \n",
    "        # Save sequence metadata\n",
    "        seq_metadata = []\n",
    "        for seq_id, seq in sequences.items():\n",
    "            seq_metadata.append({\n",
    "                'Sequence_ID': seq_id,\n",
    "                'Length': len(seq),\n",
    "                'First_10_AA': seq[:10],\n",
    "                'Last_10_AA': seq[-10:]\n",
    "            })\n",
    "        \n",
    "        metadata_df = pd.DataFrame(seq_metadata)\n",
    "        file_manager.save_file(metadata_df, 'query_metadata.csv', 'csv_outputs')\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\n‚úÖ Extracted {len(sequences)} sequence(s) from PDB {pdb_id}\")\n",
    "        display(metadata_df)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_manager.save_checkpoint('sequences', {\n",
    "            'pdb_id': pdb_id,\n",
    "            'sequences': sequences,\n",
    "            'query_file': query_file\n",
    "        })\n",
    "        \n",
    "        progress_tracker.complete_step('Sequence Extraction')\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(sequences)} sequences\")\n",
    "        return sequences, query_file\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"No sequences extracted\")\n",
    "\n",
    "# Run extraction\n",
    "sequences, query_file = download_and_extract_sequences()\n",
    "print(f\"\\n‚úÖ Query sequences saved to: {query_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## CELL 6: MMseqs2 Homolog Search",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MMSEQS2 HOMOLOG SEARCH\n# ============================================================================\n\n@safe_cell_execution(\"MMseqs2 Homolog Search\")\ndef run_mmseqs2_search():\n    \"\"\"Run MMseqs2 search for homologs and paralogs\"\"\"\n    \n    if not config or not sequences:\n        raise ValueError(\"Please run previous cells first!\")\n    \n    progress_tracker.start_step('Homolog Search')\n    \n    logger.info(f\"Starting MMseqs2 search with database {config['database']}\")\n    \n    # Check if MMseqs2 is available\n    mmseqs_bin = '/content/tools/mmseqs/bin/mmseqs'\n    if not os.path.exists(mmseqs_bin):\n        logger.warning(\"MMseqs2 binary not found, attempting to install\")\n        # Try to install MMseqs2\n        try:\n            os.makedirs('/content/tools', exist_ok=True)\n            install_cmd = '''\n            cd /content/tools && \\\n            wget -q https://github.com/soedinglab/MMseqs2/releases/latest/download/mmseqs-linux-avx2.tar.gz && \\\n            tar xzf mmseqs-linux-avx2.tar.gz\n            '''\n            subprocess.run(install_cmd, shell=True, check=True)\n            logger.info(\"MMseqs2 installed successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to install MMseqs2: {e}\")\n            print(\"‚ö†Ô∏è MMseqs2 not available, using mock data for demonstration\")\n            return generate_mock_homologs()\n    \n    # Download database if needed\n    db_dir = '/content/databases'\n    os.makedirs(db_dir, exist_ok=True)\n    \n    db_name = config['database']\n    db_path = f'{db_dir}/{db_name}'\n    \n    if not os.path.exists(db_path):\n        print(f\"üì• Downloading {db_name} database (this may take a while)...\")\n        logger.info(f\"Downloading {db_name} database\")\n        \n        try:\n            # For demo purposes, use a smaller database or subset\n            if config['test_mode']:\n                print(\"üß™ Test mode: Using small test database\")\n                # Create a small test database\n                subprocess.run(\n                    f'{mmseqs_bin} databases UniProtKB/Swiss-Prot {db_path} /tmp',\n                    shell=True,\n                    capture_output=True\n                )\n            else:\n                # Download full database\n                subprocess.run(\n                    f'{mmseqs_bin} databases {db_name} {db_path} /tmp',\n                    shell=True,\n                    capture_output=True,\n                    timeout=1800  # 30 minute timeout\n                )\n            \n            logger.info(f\"{db_name} database ready\")\n        except subprocess.TimeoutExpired:\n            logger.warning(\"Database download timeout, using fallback\")\n            print(\"‚ö†Ô∏è Database download timeout, using smaller database\")\n            db_name = 'UniProtKB/Swiss-Prot'\n            db_path = f'{db_dir}/SwissProt'\n        except Exception as e:\n            logger.error(f\"Database download failed: {e}\")\n            print(f\"‚ö†Ô∏è Could not download database: {e}\")\n            return generate_mock_homologs()\n    \n    # Run MMseqs2 search\n    try:\n        print(\"üîç Running homolog search...\")\n        \n        # Create query database\n        query_db = '/content/temp/query_db'\n        result_db = '/content/temp/result_db'\n        \n        # Convert FASTA to MMseqs2 database\n        subprocess.run(\n            f'{mmseqs_bin} createdb {query_file} {query_db}',\n            shell=True,\n            check=True,\n            capture_output=True\n        )\n        \n        # Run search\n        search_params = [\n            f'-e {config[\"evalue\"]}',\n            f'--max-seqs {config[\"max_sequences\"]}',\n            f'--min-seq-id 0.3',\n            f'-c {config[\"min_coverage\"]/100}',\n            '--threads 4'\n        ]\n        \n        search_cmd = f'{mmseqs_bin} search {query_db} {db_path} {result_db} /tmp {\" \".join(search_params)}'\n        \n        logger.debug(f\"Search command: {search_cmd}\")\n        \n        result = subprocess.run(\n            search_cmd,\n            shell=True,\n            capture_output=True,\n            text=True,\n            timeout=600  # 10 minute timeout\n        )\n        \n        if result.returncode != 0:\n            logger.warning(f\"Search returned non-zero: {result.stderr}\")\n        \n        # Convert results to FASTA\n        result_fasta = '/content/temp/homologs.fasta'\n        subprocess.run(\n            f'{mmseqs_bin} result2flat {query_db} {db_path} {result_db} {result_fasta}',\n            shell=True,\n            check=True\n        )\n        \n        # Parse results\n        homologs = parse_mmseqs2_results(result_fasta)\n        \n        print(f\"‚úÖ Found {len(homologs)} homologous sequences\")\n        logger.info(f\"Search complete: {len(homologs)} homologs found\")\n        \n    except subprocess.TimeoutExpired:\n        logger.warning(\"Search timeout, using partial results\")\n        print(\"‚ö†Ô∏è Search timeout, using partial results\")\n        homologs = generate_mock_homologs()\n    except Exception as e:\n        logger.error(f\"Search failed: {e}\")\n        print(f\"‚ö†Ô∏è Search failed: {e}\")\n        print(\"Using mock data for demonstration\")\n        homologs = generate_mock_homologs()\n    \n    # Save homolog data\n    save_homolog_results(homologs)\n    \n    # Create visualizations\n    create_homolog_visualizations(homologs)\n    \n    progress_tracker.complete_step('Homolog Search')\n    \n    return homologs\n\ndef parse_mmseqs2_results(result_file):\n    \"\"\"Parse MMseqs2 search results\"\"\"\n    homologs = []\n    \n    try:\n        if os.path.exists(result_file):\n            with open(result_file, 'r') as f:\n                for record in SeqIO.parse(f, 'fasta'):\n                    homologs.append({\n                        'id': record.id,\n                        'sequence': str(record.seq),\n                        'description': record.description\n                    })\n    except Exception as e:\n        logger.error(f\"Failed to parse results: {e}\")\n    \n    return homologs\n\ndef generate_mock_homologs():\n    \"\"\"Generate mock homolog data for testing\"\"\"\n    import random\n    \n    logger.info(\"Generating mock homolog data\")\n    \n    # Get the first query sequence\n    query_seq = list(sequences.values())[0]\n    \n    homologs = []\n    organisms = ['Human', 'Mouse', 'Rat', 'Zebrafish', 'Drosophila', 'C.elegans', 'Yeast']\n    \n    # Add query sequence\n    homologs.append({\n        'id': f'{config[\"pdb_id\"]}_query',\n        'sequence': query_seq,\n        'description': 'Query sequence',\n        'organism': 'Query',\n        'evalue': 0.0,\n        'identity': 100.0\n    })\n    \n    # Generate mock homologs with mutations\n    for i in range(min(20, config['max_sequences'])):\n        # Create sequence with random mutations\n        seq_list = list(query_seq)\n        num_mutations = random.randint(5, len(seq_list)//3)\n        \n        for _ in range(num_mutations):\n            pos = random.randint(0, len(seq_list)-1)\n            aa_choices = 'ACDEFGHIKLMNPQRSTVWY'\n            seq_list[pos] = random.choice(aa_choices)\n        \n        homologs.append({\n            'id': f'homolog_{i+1}',\n            'sequence': ''.join(seq_list),\n            'description': f'{random.choice(organisms)} protein homolog',\n            'organism': random.choice(organisms),\n            'evalue': 10**(random.uniform(-10, -3)),\n            'identity': random.uniform(30, 95)\n        })\n    \n    return homologs\n\ndef save_homolog_results(homologs):\n    \"\"\"Save homolog search results\"\"\"\n    \n    # Save as FASTA\n    fasta_content = \"\"\n    for h in homologs:\n        fasta_content += f\">{h['id']} {h.get('description', '')}\\n{h['sequence']}\\n\"\n    \n    homolog_fasta = file_manager.save_file(fasta_content, 'homologs.fasta', 'sequences')\n    \n    # Save summary CSV\n    summary_data = []\n    for h in homologs:\n        summary_data.append({\n            'ID': h['id'],\n            'Description': h.get('description', ''),\n            'Length': len(h['sequence']),\n            'E-value': h.get('evalue', 'N/A'),\n            'Identity_%': h.get('identity', 'N/A'),\n            'Organism': h.get('organism', 'Unknown')\n        })\n    \n    summary_df = pd.DataFrame(summary_data)\n    file_manager.save_file(summary_df, 'homolog_summary.csv', 'csv_outputs')\n    \n    # Display top results\n    print(\"\\nüìä Top 10 Homologs:\")\n    display(summary_df.head(10))\n    \n    # Save checkpoint\n    checkpoint_manager.save_checkpoint('homologs', {\n        'count': len(homologs),\n        'homolog_fasta': homolog_fasta\n    })\n\ndef create_homolog_visualizations(homologs):\n    \"\"\"Create visualizations for homolog search results\"\"\"\n    \n    if len(homologs) == 0:\n        return\n    \n    # E-value distribution\n    if any('evalue' in h for h in homologs):\n        evalues = [h.get('evalue', 1e-3) for h in homologs if h.get('evalue')]\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # E-value distribution (log scale)\n        ax1.hist(np.log10(evalues), bins=30, edgecolor='black')\n        ax1.set_xlabel('log10(E-value)')\n        ax1.set_ylabel('Count')\n        ax1.set_title('E-value Distribution')\n        ax1.grid(True, alpha=0.3)\n        \n        # Identity distribution\n        if any('identity' in h for h in homologs):\n            identities = [h.get('identity', 50) for h in homologs if h.get('identity')]\n            ax2.hist(identities, bins=30, edgecolor='black', color='green')\n            ax2.set_xlabel('Identity (%)')\n            ax2.set_ylabel('Count')\n            ax2.set_title('Sequence Identity Distribution')\n            ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(file_manager.dirs['visualizations'] + '/homolog_distributions.png', dpi=150)\n        plt.show()\n    \n    # Organism distribution pie chart\n    if any('organism' in h for h in homologs):\n        organisms = [h.get('organism', 'Unknown') for h in homologs]\n        org_counts = pd.Series(organisms).value_counts()\n        \n        if len(org_counts) > 0:\n            fig, ax = plt.subplots(figsize=(10, 8))\n            colors = plt.cm.Set3(np.linspace(0, 1, len(org_counts)))\n            ax.pie(org_counts.values, labels=org_counts.index, autopct='%1.1f%%', colors=colors)\n            ax.set_title('Organism Distribution in Homologs')\n            plt.savefig(file_manager.dirs['visualizations'] + '/organism_distribution.png', dpi=150)\n            plt.show()\n\n# Run homolog search\nhomologs = run_mmseqs2_search()\nprint(f\"\\n‚úÖ Homolog search complete! Found {len(homologs)} sequences.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## CELL 7: Multiple Sequence Alignment with FAMSA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MULTIPLE SEQUENCE ALIGNMENT WITH FAMSA\n# ============================================================================\n\n@safe_cell_execution(\"Multiple Sequence Alignment\")\ndef perform_alignment():\n    \"\"\"Perform multiple sequence alignment using FAMSA or fallback methods\"\"\"\n    \n    if not homologs:\n        raise ValueError(\"Please run homolog search first!\")\n    \n    progress_tracker.start_step('Alignment')\n    \n    logger.info(f\"Starting alignment with {len(homologs)} sequences\")\n    print(f\"üß¨ Aligning {len(homologs)} sequences...\")\n    \n    alignment = None\n    \n    # Method 1: Try pyfamsa\n    try:\n        import pyfamsa\n        logger.info(\"Using pyfamsa for alignment\")\n        \n        # Convert homologs to pyfamsa format\n        sequences_list = []\n        for h in homologs:\n            sequences_list.append(pyfamsa.Sequence(\n                h['id'].encode(),\n                h['sequence'].encode()\n            ))\n        \n        # Run FAMSA alignment\n        aligner = pyfamsa.Aligner(threads=4)\n        msa = aligner.align(sequences_list)\n        \n        # Convert to Biopython alignment\n        from Bio.Align import MultipleSeqAlignment\n        from Bio.SeqRecord import SeqRecord\n        from Bio.Seq import Seq\n        \n        aligned_records = []\n        for seq in msa:\n            aligned_records.append(SeqRecord(\n                Seq(seq.sequence.decode()),\n                id=seq.id.decode(),\n                description=''\n            ))\n        \n        alignment = MultipleSeqAlignment(aligned_records)\n        logger.info(\"FAMSA alignment successful\")\n        \n    except ImportError:\n        logger.warning(\"pyfamsa not available, trying alternative methods\")\n        \n        # Method 2: Try MAFFT via subprocess\n        try:\n            logger.info(\"Attempting MAFFT alignment\")\n            \n            # Save sequences to temp file\n            temp_input = '/content/temp/input.fasta'\n            temp_output = '/content/temp/aligned.fasta'\n            \n            with open(temp_input, 'w') as f:\n                for h in homologs:\n                    f.write(f\">{h['id']}\\n{h['sequence']}\\n\")\n            \n            # Try to install MAFFT if not present\n            mafft_check = subprocess.run('which mafft', shell=True, capture_output=True)\n            if mafft_check.returncode != 0:\n                subprocess.run('apt-get update && apt-get install -y mafft', shell=True)\n            \n            # Run MAFFT\n            subprocess.run(\n                f'mafft --auto --thread 4 {temp_input} > {temp_output}',\n                shell=True,\n                check=True\n            )\n            \n            # Load alignment\n            alignment = AlignIO.read(temp_output, 'fasta')\n            logger.info(\"MAFFT alignment successful\")\n            \n        except Exception as e:\n            logger.warning(f\"MAFFT failed: {e}, using simple alignment\")\n            \n            # Method 3: Simple pairwise alignment fallback\n            alignment = create_simple_alignment(homologs)\n    \n    except Exception as e:\n        logger.error(f\"Alignment failed: {e}\")\n        print(f\"‚ö†Ô∏è Alignment error: {e}\")\n        # Use simple alignment as last resort\n        alignment = create_simple_alignment(homologs)\n    \n    if alignment:\n        # Save alignment\n        alignment_file = file_manager.dirs['alignments'] + '/alignment.fasta'\n        AlignIO.write(alignment, alignment_file, 'fasta')\n        logger.info(f\"Alignment saved to {alignment_file}\")\n        \n        # Calculate alignment statistics\n        stats = calculate_alignment_stats(alignment)\n        \n        # Save statistics\n        stats_df = pd.DataFrame([stats])\n        file_manager.save_file(stats_df, 'alignment_statistics.csv', 'csv_outputs')\n        \n        # Display statistics\n        print(\"\\nüìä Alignment Statistics:\")\n        print(f\"  ‚Ä¢ Number of sequences: {alignment.get_alignment_length()}\")\n        print(f\"  ‚Ä¢ Alignment length: {len(alignment[0])}\")\n        print(f\"  ‚Ä¢ Conservation score: {stats['conservation']:.2%}\")\n        print(f\"  ‚Ä¢ Gap percentage: {stats['gap_percentage']:.2%}\")\n        \n        # Create alignment visualization\n        visualize_alignment(alignment)\n        \n        # Save checkpoint\n        checkpoint_manager.save_checkpoint('alignment', {\n            'num_sequences': len(alignment),\n            'alignment_length': alignment.get_alignment_length(),\n            'alignment_file': alignment_file\n        })\n        \n        progress_tracker.complete_step('Alignment')\n        \n        return alignment\n    \n    else:\n        raise Exception(\"Failed to create alignment\")\n\ndef create_simple_alignment(homologs):\n    \"\"\"Create a simple alignment for fallback\"\"\"\n    from Bio.Align import MultipleSeqAlignment\n    from Bio.SeqRecord import SeqRecord\n    from Bio.Seq import Seq\n    \n    logger.info(\"Creating simple alignment as fallback\")\n    \n    # Find the maximum sequence length\n    max_len = max(len(h['sequence']) for h in homologs)\n    \n    # Pad sequences to same length\n    aligned_records = []\n    for h in homologs:\n        padded_seq = h['sequence'] + '-' * (max_len - len(h['sequence']))\n        aligned_records.append(SeqRecord(\n            Seq(padded_seq),\n            id=h['id'],\n            description=''\n        ))\n    \n    return MultipleSeqAlignment(aligned_records)\n\ndef calculate_alignment_stats(alignment):\n    \"\"\"Calculate alignment statistics\"\"\"\n    \n    stats = {}\n    \n    # Get alignment dimensions\n    num_seqs = len(alignment)\n    aln_length = alignment.get_alignment_length()\n    \n    # Calculate conservation\n    conservation_scores = []\n    gap_counts = []\n    \n    for i in range(aln_length):\n        column = alignment[:, i]\n        \n        # Count gaps\n        gap_count = column.count('-')\n        gap_counts.append(gap_count)\n        \n        # Calculate conservation (excluding gaps)\n        non_gap_chars = [c for c in column if c != '-']\n        if non_gap_chars:\n            most_common = max(set(non_gap_chars), key=non_gap_chars.count)\n            conservation = non_gap_chars.count(most_common) / len(non_gap_chars)\n            conservation_scores.append(conservation)\n    \n    stats['num_sequences'] = num_seqs\n    stats['alignment_length'] = aln_length\n    stats['conservation'] = np.mean(conservation_scores) if conservation_scores else 0\n    stats['gap_percentage'] = np.mean(gap_counts) / num_seqs if gap_counts else 0\n    stats['highly_conserved_positions'] = sum(1 for s in conservation_scores if s > 0.9)\n    \n    return stats\n\ndef visualize_alignment(alignment):\n    \"\"\"Create alignment visualization\"\"\"\n    \n    # Conservation plot\n    aln_length = alignment.get_alignment_length()\n    conservation_scores = []\n    \n    for i in range(min(aln_length, 500)):  # Limit to first 500 positions for visualization\n        column = alignment[:, i]\n        non_gap_chars = [c for c in column if c != '-']\n        if non_gap_chars:\n            most_common = max(set(non_gap_chars), key=non_gap_chars.count)\n            conservation = non_gap_chars.count(most_common) / len(non_gap_chars)\n            conservation_scores.append(conservation)\n        else:\n            conservation_scores.append(0)\n    \n    # Create conservation plot\n    fig, ax = plt.subplots(figsize=(14, 4))\n    positions = range(len(conservation_scores))\n    ax.plot(positions, conservation_scores, linewidth=1)\n    ax.fill_between(positions, conservation_scores, alpha=0.3)\n    ax.set_xlabel('Position')\n    ax.set_ylabel('Conservation Score')\n    ax.set_title('Alignment Conservation Profile')\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim([0, 1])\n    \n    # Add threshold line\n    ax.axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='High conservation (>90%)')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig(file_manager.dirs['visualizations'] + '/alignment_conservation.png', dpi=150)\n    plt.show()\n    \n    # Create a heatmap of the first 20 sequences and 100 positions\n    if len(alignment) > 1:\n        # Convert alignment to numeric matrix for heatmap\n        aa_to_num = {aa: i for i, aa in enumerate('ACDEFGHIKLMNPQRSTVWY-')}\n        \n        num_seqs_show = min(20, len(alignment))\n        num_pos_show = min(100, alignment.get_alignment_length())\n        \n        matrix = np.zeros((num_seqs_show, num_pos_show))\n        \n        for i in range(num_seqs_show):\n            for j in range(num_pos_show):\n                aa = alignment[i].seq[j]\n                matrix[i, j] = aa_to_num.get(aa, 20)\n        \n        # Create heatmap\n        fig, ax = plt.subplots(figsize=(14, 6))\n        im = ax.imshow(matrix, cmap='viridis', aspect='auto')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Sequence')\n        ax.set_title('Alignment Heatmap (subset)')\n        \n        # Add sequence IDs as y-axis labels\n        seq_ids = [alignment[i].id[:20] for i in range(num_seqs_show)]\n        ax.set_yticks(range(num_seqs_show))\n        ax.set_yticklabels(seq_ids, fontsize=8)\n        \n        plt.colorbar(im, ax=ax, label='Amino Acid')\n        plt.tight_layout()\n        plt.savefig(file_manager.dirs['visualizations'] + '/alignment_heatmap.png', dpi=150)\n        plt.show()\n\n# Perform alignment\nalignment = perform_alignment()\nprint(\"\\n‚úÖ Alignment complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## CELL 8: Phylogenetic Tree Construction with IQ-TREE",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# PHYLOGENETIC TREE CONSTRUCTION WITH IQ-TREE\n# ============================================================================\n\n@safe_cell_execution(\"Phylogenetic Tree Construction\")\ndef construct_phylogenetic_tree():\n    \"\"\"Construct phylogenetic tree using IQ-TREE or alternatives\"\"\"\n    \n    if not alignment:\n        raise ValueError(\"Please run alignment first!\")\n    \n    progress_tracker.start_step('Tree Construction')\n    \n    logger.info(\"Starting phylogenetic tree construction\")\n    print(\"üå≥ Constructing phylogenetic tree...\")\n    \n    tree = None\n    \n    # Save alignment for tree construction\n    aln_file = '/content/temp/alignment_for_tree.fasta'\n    AlignIO.write(alignment, aln_file, 'fasta')\n    \n    # Method 1: Try IQ-TREE\n    iqtree_bin = '/content/tools/iqtree2'\n    if os.path.exists(iqtree_bin):\n        try:\n            logger.info(\"Using IQ-TREE for tree construction\")\n            \n            # Run IQ-TREE with ModelFinder and UFBoot\n            iqtree_cmd = [\n                iqtree_bin,\n                '-s', aln_file,\n                '-m', 'MFP',  # ModelFinder\n                '-bb', '1000',  # UFBoot with 1000 replicates\n                '-alrt', '1000',  # SH-aLRT test\n                '-nt', '4',  # Number of threads\n                '-pre', '/content/temp/iqtree'\n            ]\n            \n            result = subprocess.run(\n                iqtree_cmd,\n                capture_output=True,\n                text=True,\n                timeout=600\n            )\n            \n            if result.returncode == 0:\n                # Load the tree\n                tree_file = '/content/temp/iqtree.treefile'\n                if os.path.exists(tree_file):\n                    tree = Phylo.read(tree_file, 'newick')\n                    logger.info(\"IQ-TREE completed successfully\")\n                    \n                    # Parse IQ-TREE report for best model\n                    report_file = '/content/temp/iqtree.iqtree'\n                    if os.path.exists(report_file):\n                        with open(report_file, 'r') as f:\n                            report_content = f.read()\n                            if 'Best-fit model:' in report_content:\n                                best_model = report_content.split('Best-fit model:')[1].split('\\n')[0].strip()\n                                print(f\"üìä Best-fit model: {best_model}\")\n            else:\n                logger.warning(f\"IQ-TREE failed: {result.stderr}\")\n                \n        except subprocess.TimeoutExpired:\n            logger.warning(\"IQ-TREE timeout\")\n        except Exception as e:\n            logger.error(f\"IQ-TREE error: {e}\")\n    \n    # Method 2: Try FastTree\n    if tree is None:\n        try:\n            logger.info(\"Trying FastTree\")\n            \n            # Install FastTree if needed\n            fasttree_check = subprocess.run('which fasttree', shell=True, capture_output=True)\n            if fasttree_check.returncode != 0:\n                subprocess.run('apt-get update && apt-get install -y fasttree', shell=True)\n            \n            # Run FastTree\n            tree_file = '/content/temp/fasttree.tree'\n            subprocess.run(\n                f'fasttree {aln_file} > {tree_file}',\n                shell=True,\n                check=True\n            )\n            \n            if os.path.exists(tree_file):\n                tree = Phylo.read(tree_file, 'newick')\n                logger.info(\"FastTree completed successfully\")\n                \n        except Exception as e:\n            logger.error(f\"FastTree failed: {e}\")\n    \n    # Method 3: Use Biopython's distance-based method\n    if tree is None:\n        try:\n            logger.info(\"Using Biopython distance-based tree construction\")\n            \n            from Bio.Phylo.TreeConstruction import DistanceCalculator, DistanceTreeConstructor\n            \n            # Calculate distance matrix\n            calculator = DistanceCalculator('identity')\n            dm = calculator.get_distance(alignment)\n            \n            # Construct tree using UPGMA\n            constructor = DistanceTreeConstructor()\n            tree = constructor.upgma(dm)\n            \n            logger.info(\"Distance-based tree construction successful\")\n            \n        except Exception as e:\n            logger.error(f\"Distance-based construction failed: {e}\")\n            print(f\"‚ùå Could not construct tree: {e}\")\n            return None\n    \n    if tree:\n        # Save tree\n        tree_newick = file_manager.dirs['trees'] + '/phylogenetic_tree.newick'\n        Phylo.write(tree, tree_newick, 'newick')\n        logger.info(f\"Tree saved to {tree_newick}\")\n        \n        # Calculate tree statistics\n        stats = calculate_tree_stats(tree)\n        \n        # Save tree statistics\n        stats_df = pd.DataFrame([stats])\n        file_manager.save_file(stats_df, 'tree_statistics.csv', 'csv_outputs')\n        \n        # Display tree statistics\n        print(\"\\nüìä Tree Statistics:\")\n        print(f\"  ‚Ä¢ Number of terminals: {stats['num_terminals']}\")\n        print(f\"  ‚Ä¢ Number of internal nodes: {stats['num_internal']}\")\n        print(f\"  ‚Ä¢ Total branch length: {stats['total_branch_length']:.4f}\")\n        print(f\"  ‚Ä¢ Tree depth: {stats['max_depth']:.4f}\")\n        \n        # Visualize tree\n        visualize_tree(tree)\n        \n        # Save checkpoint\n        checkpoint_manager.save_checkpoint('tree', {\n            'tree_file': tree_newick,\n            'statistics': stats\n        })\n        \n        progress_tracker.complete_step('Tree Construction')\n        \n        return tree\n    \n    else:\n        raise Exception(\"Failed to construct phylogenetic tree\")\n\ndef calculate_tree_stats(tree):\n    \"\"\"Calculate tree statistics\"\"\"\n    \n    stats = {}\n    \n    # Count terminals and internal nodes\n    terminals = tree.get_terminals()\n    all_clades = list(tree.find_clades())\n    \n    stats['num_terminals'] = len(terminals)\n    stats['num_internal'] = len(all_clades) - len(terminals)\n    \n    # Calculate total branch length\n    total_length = sum(clade.branch_length for clade in all_clades if clade.branch_length)\n    stats['total_branch_length'] = total_length\n    \n    # Calculate tree depth\n    depths = [tree.distance(terminal) for terminal in terminals]\n    stats['max_depth'] = max(depths) if depths else 0\n    stats['avg_depth'] = np.mean(depths) if depths else 0\n    \n    return stats\n\ndef visualize_tree(tree):\n    \"\"\"Create tree visualizations\"\"\"\n    \n    # Basic tree plot using matplotlib\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n    \n    # Rectangular tree\n    Phylo.draw(tree, axes=ax1, do_show=False)\n    ax1.set_title('Phylogenetic Tree (Rectangular)')\n    ax1.set_xlabel('Branch Length')\n    \n    # Circular tree\n    try:\n        Phylo.draw_graphviz(tree, axes=ax2, prog='neato')\n        ax2.set_title('Phylogenetic Tree (Circular)')\n    except:\n        # If graphviz not available, draw another rectangular\n        Phylo.draw(tree, axes=ax2, do_show=False)\n        ax2.set_title('Phylogenetic Tree (Alternative View)')\n    \n    plt.tight_layout()\n    plt.savefig(file_manager.dirs['visualizations'] + '/phylogenetic_trees.png', dpi=200)\n    plt.show()\n    \n    # Create an interactive tree plot using plotly\n    try:\n        create_interactive_tree(tree)\n    except Exception as e:\n        logger.warning(f\"Could not create interactive tree: {e}\")\n\ndef create_interactive_tree(tree):\n    \"\"\"Create an interactive tree visualization with plotly\"\"\"\n    \n    # Get coordinates for tree nodes\n    def get_coords(clade, x_parent=0, depth=0):\n        coords = []\n        y = depth\n        \n        if clade.branch_length:\n            x = x_parent + clade.branch_length\n        else:\n            x = x_parent + 0.1\n        \n        coords.append((x, y, clade.name if clade.name else ''))\n        \n        for i, child in enumerate(clade.clades):\n            child_coords = get_coords(child, x, depth - (i+1)*0.5)\n            coords.extend(child_coords)\n        \n        return coords\n    \n    coords = get_coords(tree.root)\n    \n    # Create plotly figure\n    fig = go.Figure()\n    \n    # Add tree branches\n    for coord in coords:\n        x, y, label = coord\n        fig.add_trace(go.Scatter(\n            x=[x], y=[y],\n            mode='markers+text',\n            text=[label],\n            textposition='middle right',\n            marker=dict(size=5),\n            showlegend=False\n        ))\n    \n    fig.update_layout(\n        title='Interactive Phylogenetic Tree',\n        xaxis_title='Evolutionary Distance',\n        yaxis_title='',\n        height=800,\n        hovermode='closest'\n    )\n    \n    # Save interactive plot\n    fig.write_html(file_manager.dirs['visualizations'] + '/interactive_tree.html')\n    fig.show()\n\n# Construct tree\ntree = construct_phylogenetic_tree()\nprint(\"\\n‚úÖ Phylogenetic tree construction complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## CELL 9: Final Report and Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# FINAL REPORT AND SUMMARY\n# ============================================================================\n\n@safe_cell_execution(\"Final Report Generation\")\ndef generate_final_report():\n    \"\"\"Generate comprehensive final report\"\"\"\n    \n    progress_tracker.start_step('Report Generation')\n    \n    logger.info(\"Generating final report\")\n    print(\"üìù Generating final report...\")\n    \n    # Compile all results\n    report = {\n        'Analysis Summary': {\n            'PDB ID': config['pdb_id'],\n            'Analysis Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'Database Used': config['database'],\n            'E-value Threshold': config['evalue'],\n            'Output Directory': file_manager.base_dir\n        },\n        'Results': {\n            'Query Sequences': len(sequences),\n            'Homologs Found': len(homologs) if homologs else 0,\n            'Alignment Length': alignment.get_alignment_length() if alignment else 0,\n            'Tree Terminals': len(tree.get_terminals()) if tree else 0\n        }\n    }\n    \n    # Save comprehensive report\n    report_file = file_manager.save_file(report, 'analysis_report.json')\n    \n    # Create summary CSV\n    summary_data = []\n    for category, items in report.items():\n        for key, value in items.items():\n            summary_data.append({\n                'Category': category,\n                'Metric': key,\n                'Value': str(value)\n            })\n    \n    summary_df = pd.DataFrame(summary_data)\n    file_manager.save_file(summary_df, 'analysis_summary.csv', 'csv_outputs')\n    \n    # Get execution summary\n    execution_summary = progress_tracker.get_summary()\n    file_manager.save_file(execution_summary, 'execution_summary.csv', 'csv_outputs')\n    \n    # Display final summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHYLOGENETIC ANALYSIS COMPLETE\")\n    print(\"=\"*70)\n    \n    print(\"\\nüìä Analysis Summary:\")\n    for category, items in report.items():\n        print(f\"\\n{category}:\")\n        for key, value in items.items():\n            print(f\"  ‚Ä¢ {key}: {value}\")\n    \n    print(\"\\n‚è±Ô∏è Execution Times:\")\n    display(execution_summary)\n    \n    print(\"\\nüìÅ Output Files:\")\n    for dir_name, dir_path in file_manager.dirs.items():\n        files = os.listdir(dir_path)\n        if files:\n            print(f\"\\n  {dir_name}:\")\n            for f in files[:5]:  # Show first 5 files\n                print(f\"    - {f}\")\n            if len(files) > 5:\n                print(f\"    ... and {len(files)-5} more files\")\n    \n    # Create download links if in Colab\n    if IN_COLAB:\n        print(\"\\nüíæ Download Results:\")\n        print(f\"Results saved to: {file_manager.base_dir}\")\n        \n        # Create a zip file of all results\n        try:\n            import zipfile\n            zip_path = f'/content/{config[\"pdb_id\"]}_phylogenetic_results.zip'\n            \n            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                for root, dirs, files in os.walk(file_manager.base_dir):\n                    for file in files:\n                        file_path = os.path.join(root, file)\n                        arcname = os.path.relpath(file_path, file_manager.base_dir)\n                        zipf.write(file_path, arcname)\n            \n            print(f\"\\nüì¶ Results archived: {zip_path}\")\n            \n            # Offer download\n            from google.colab import files\n            print(\"\\n‚¨áÔ∏è Download the complete results archive:\")\n            files.download(zip_path)\n            \n        except Exception as e:\n            logger.error(f\"Could not create zip archive: {e}\")\n    \n    progress_tracker.complete_step('Report Generation')\n    \n    # Final log message\n    logger.info(\"Analysis pipeline completed successfully\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"‚úÖ ANALYSIS COMPLETE!\")\n    print(\"=\"*70)\n    print(\"\\nThank you for using the Phylogenetic Analysis Pipeline!\")\n    print(\"Results have been saved to your Google Drive.\")\n    \n    return report\n\n# Generate final report\nfinal_report = generate_final_report()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}